{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:35:28.569426Z","iopub.execute_input":"2025-06-24T16:35:28.570028Z","iopub.status.idle":"2025-06-24T16:35:28.875799Z","shell.execute_reply.started":"2025-06-24T16:35:28.570004Z","shell.execute_reply":"2025-06-24T16:35:28.875282Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install datasets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:35:28.876758Z","iopub.execute_input":"2025-06-24T16:35:28.877044Z","iopub.status.idle":"2025-06-24T16:35:33.333788Z","shell.execute_reply.started":"2025-06-24T16:35:28.877026Z","shell.execute_reply":"2025-06-24T16:35:33.333080Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"roneneldan/TinyStories\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:35:33.334760Z","iopub.execute_input":"2025-06-24T16:35:33.334982Z","iopub.status.idle":"2025-06-24T16:35:50.562178Z","shell.execute_reply.started":"2025-06-24T16:35:33.334959Z","shell.execute_reply":"2025-06-24T16:35:50.561624Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcadd7c95b14424c84fa3972ff7bb5fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00004-2d5a1467fff1081b.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c07f7a6041f4a5d8313d73871cb284d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00001-of-00004-5852b56a2bd28fd9.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5336cad39d94451bb3603590c0537714"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00002-of-00004-a26307300439e943.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f28fa642ce84aa28e05d094bd750ffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00003-of-00004-d243063613e5a057.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32f24a2ac754470192247921b899468f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-869c898b519ad725.parquet:   0%|          | 0.00/9.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32b88910a3d14d40877641b3e3fbe31a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fa50fa2cc714a85bcdd48e5a80276ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d63f4f7878e1423d9ffaaed30f1ecf3a"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!pip install tiktoken","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:35:50.563742Z","iopub.execute_input":"2025-06-24T16:35:50.564113Z","iopub.status.idle":"2025-06-24T16:35:53.819602Z","shell.execute_reply.started":"2025-06-24T16:35:50.564094Z","shell.execute_reply":"2025-06-24T16:35:53.818582Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import tiktoken\nimport os\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom dataclasses import dataclass\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom contextlib import nullcontext\nimport os\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:35:53.820795Z","iopub.execute_input":"2025-06-24T16:35:53.821058Z","iopub.status.idle":"2025-06-24T16:35:55.701184Z","shell.execute_reply.started":"2025-06-24T16:35:53.821033Z","shell.execute_reply":"2025-06-24T16:35:55.700622Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"enc = tiktoken.get_encoding(\"gpt2\")\n\ndef process(example):\n    ids = enc.encode_ordinary(example['text'])\n    out = {'ids': ids, 'len': len(ids)}\n    return out\nif not os.path.exists(\"train.bin\"):\n    tokenized = ds.map(\n        process,\n        remove_columns=['text'],\n        desc = \"tokenizing the splits\",\n        num_proc = 8,\n    )\n    for split, dset in tokenized.items():\n        arr_len = np.sum(dset['len'], dtype = np.uint64)\n        filename = f'{split}.bin'\n        dtype = np.uint16\n        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n        total_batches = 1024\n\n        idx =0 \n        for batch_idx in tqdm(range(total_batches),desc=f'writing {filename}'):\n            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True)\n            arr_batch = np.concatenate(batch['ids'])\n            arr[idx : idx+len(arr_batch)] = arr_batch\n            idx += len(arr_batch)\n        arr.flush()\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:35:55.701914Z","iopub.execute_input":"2025-06-24T16:35:55.702153Z","iopub.status.idle":"2025-06-24T16:42:41.939715Z","shell.execute_reply.started":"2025-06-24T16:35:55.702098Z","shell.execute_reply":"2025-06-24T16:42:41.938734Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizing the splits (num_proc=8):   0%|          | 0/2119719 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b637c991ad54d098b45051b5c7a2adc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizing the splits (num_proc=8):   0%|          | 0/21990 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef0acdf6737c4deba880b2f6d1de12b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"writing train.bin:   0%|          | 0/1024 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23cd2f5b09a6446b9a976fb2496460e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"writing validation.bin:   0%|          | 0/1024 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d79bcbf43e41449aac112e1897e6d22a"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"\ndef get_batch(split):\n    # We recreate np.memmap every batch to avoid a memory leak, as per\n\n    if split == 'train':\n        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n    else:\n        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if device_type == 'cuda':\n        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    return x, y\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:42:41.940664Z","iopub.execute_input":"2025-06-24T16:42:41.941005Z","iopub.status.idle":"2025-06-24T16:42:41.947103Z","shell.execute_reply.started":"2025-06-24T16:42:41.940981Z","shell.execute_reply":"2025-06-24T16:42:41.946302Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom dataclasses import dataclass\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom contextlib import nullcontext\nimport os\n\nclass LayerNorm(nn.Module):\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n    def forward(self, x):\n        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.flash = hasattr(F, 'scaled_dot_product_attention')\n        if not self.flash:\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                       .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size()\n        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n\n        if self.flash:\n            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.attn_dropout(att)\n            y = att @ v\n\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\nclass MLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu = nn.GELU()\n        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n    def forward(self, x):\n        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln1 = LayerNorm(config.n_embd, config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln2 = LayerNorm(config.n_embd, config.bias)\n        self.mlp = MLP(config)\n    def forward(self, x):\n        x = x + self.attn(self.ln1(x))\n        x = x + self.mlp(self.ln2(x))\n        return x\n\n@dataclass\nclass GPTConfig:\n    block_size: int\n    vocab_size: int\n    n_layer: int\n    n_head: int\n    n_embd: int\n    dropout: float = 0.0\n    bias: bool = True\n\nclass GPT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.transformer = nn.ModuleDict(dict(\n            wte=nn.Embedding(config.vocab_size, config.n_embd),\n            wpe=nn.Embedding(config.block_size, config.n_embd),\n            drop=nn.Dropout(config.dropout),\n            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f=LayerNorm(config.n_embd, config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n\n        self.apply(self._init_weights)\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        b, t = idx.size()\n        assert t <= self.config.block_size\n        pos = torch.arange(0, t, dtype=torch.long, device=device)\n\n        tok_emb = self.transformer.wte(idx)\n        pos_emb = self.transformer.wpe(pos)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n            return logits, loss\n        else:\n            logits = self.lm_head(x[:, [-1], :])\n            return logits, None\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Generate tokens given a conditioning sequence.\n        idx: Tensor of shape (B, T)\n        \"\"\"\n        for _ in range(max_new_tokens):\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n        return idx\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:42:41.947855Z","iopub.execute_input":"2025-06-24T16:42:41.948094Z","iopub.status.idle":"2025-06-24T16:42:41.971887Z","shell.execute_reply.started":"2025-06-24T16:42:41.948037Z","shell.execute_reply":"2025-06-24T16:42:41.971206Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"config = GPTConfig(\n    vocab_size=50257,     #use the tokenizer's vocab size\n    block_size=128,       #or whatever context size you're training with\n    n_layer=6,            #number of transformer blocks\n    n_head=6,             #number of attention heads\n    n_embd=384,\n    dropout=0.1,\n    bias=True\n)\n\nmodel = GPT(config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:42:41.972638Z","iopub.execute_input":"2025-06-24T16:42:41.972973Z","iopub.status.idle":"2025-06-24T16:42:42.760248Z","shell.execute_reply.started":"2025-06-24T16:42:41.972945Z","shell.execute_reply":"2025-06-24T16:42:42.759684Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def estimate_loss(model):\n    out = {}\n    model.eval()\n    with torch.inference_mode():\n        for split in ['train', 'val']:\n            losses = torch.zeros(eval_iters)\n            for k in range(eval_iters):\n                X, Y = get_batch(split)\n                with ctx:\n                    logits, loss = model(X, Y)\n                losses[k] = loss.item()\n            out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:42:42.762567Z","iopub.execute_input":"2025-06-24T16:42:42.762785Z","iopub.status.idle":"2025-06-24T16:42:42.767351Z","shell.execute_reply.started":"2025-06-24T16:42:42.762767Z","shell.execute_reply":"2025-06-24T16:42:42.766676Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Training Config\nimport torch\nfrom contextlib import nullcontext\n\nlearning_rate = 1e-4 \nmax_iters = 20000 \nwarmup_steps = 1000 #smoother initial train, earlier 100\nmin_lr = 5e-4 \neval_iters = 500 # increased from 100\nbatch_size = 32 # changed from 16, better gradient estimate\nblock_size = 128 #changed from 64, capture longer range dependencies\n\ngradient_accumulation_steps = 32 # reduced from 50\n\ndevice =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n# note: float16 data type will automatically use a GradScaler\n\n# How to use autocast https://wandb.ai/wandb_fc/tips/reports/How-To-Use-Autocast-in-PyTorch--VmlldzoyMTk4NTky\n#dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\ntorch.set_default_device(device)\ntorch.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:42:42.767965Z","iopub.execute_input":"2025-06-24T16:42:42.768163Z","iopub.status.idle":"2025-06-24T16:42:42.914616Z","shell.execute_reply.started":"2025-06-24T16:42:42.768146Z","shell.execute_reply":"2025-06-24T16:42:42.913987Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7d67786c34d0>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n\n\noptimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9) #weight decay for regularization\n\nscheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\nscheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\nscheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps]) #Switching from warmup to decay\n\n# https://stackoverflow.com/questions/72534859/is-gradscaler-necessary-with-mixed-precision-training-with-pytorch\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:42:42.915342Z","iopub.execute_input":"2025-06-24T16:42:42.915571Z","iopub.status.idle":"2025-06-24T16:42:45.848646Z","shell.execute_reply.started":"2025-06-24T16:42:42.915554Z","shell.execute_reply":"2025-06-24T16:42:45.847820Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_79/3173834881.py:11: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"best_val_loss = float('inf')\nbest_model_params_path = \"best_model_params.pt\"\ntrain_loss_list, validation_loss_list = [], []\n\n\nmodel = model.to(device)\n\n# In training loop\nfor epoch in tqdm(range(max_iters)):\n    if epoch % eval_iters == 0 and epoch != 0:\n        \n        losses = estimate_loss(model)\n        print(f\"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        print(f\"The current learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n        train_loss_list += [losses['train']]\n        validation_loss_list += [losses['val']]\n\n        if losses['val'] < best_val_loss:\n            best_val_loss = losses['val']\n            torch.save(model.state_dict(), best_model_params_path)\n\n    # Ensure X and y are on the correct device\n    X, y = get_batch(\"train\")\n    X, y = X.to(device), y.to(device)\n\n    with ctx:\n        logits, loss = model(X, y)\n        loss = loss / gradient_accumulation_steps\n        scaler.scale(loss).backward()\n\n    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad(set_to_none=True)\n    scheduler.step()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T16:42:45.849541Z","iopub.execute_input":"2025-06-24T16:42:45.850065Z","iopub.status.idle":"2025-06-24T18:21:32.567649Z","shell.execute_reply.started":"2025-06-24T16:42:45.850039Z","shell.execute_reply":"2025-06-24T18:21:32.566718Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50176bb4c1614459b83a878935281353"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 500: train loss 9.3986, val loss 9.4044\nThe current learning rate: 0.00007\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1000: train loss 8.4447, val loss 8.4480\nThe current learning rate: 0.00010\nEpoch 1500: train loss 7.4957, val loss 7.4892\nThe current learning rate: 0.00010\nEpoch 2000: train loss 6.6601, val loss 6.6536\nThe current learning rate: 0.00010\nEpoch 2500: train loss 5.9653, val loss 5.9604\nThe current learning rate: 0.00011\nEpoch 3000: train loss 5.4669, val loss 5.4598\nThe current learning rate: 0.00011\nEpoch 3500: train loss 5.0575, val loss 5.0556\nThe current learning rate: 0.00012\nEpoch 4000: train loss 4.7356, val loss 4.7309\nThe current learning rate: 0.00012\nEpoch 4500: train loss 4.4933, val loss 4.4971\nThe current learning rate: 0.00013\nEpoch 5000: train loss 4.2785, val loss 4.2757\nThe current learning rate: 0.00014\nEpoch 5500: train loss 4.1125, val loss 4.1045\nThe current learning rate: 0.00015\nEpoch 6000: train loss 3.9526, val loss 3.9582\nThe current learning rate: 0.00016\nEpoch 6500: train loss 3.8191, val loss 3.8170\nThe current learning rate: 0.00018\nEpoch 7000: train loss 3.7161, val loss 3.7177\nThe current learning rate: 0.00019\nEpoch 7500: train loss 3.5951, val loss 3.5956\nThe current learning rate: 0.00020\nEpoch 8000: train loss 3.4972, val loss 3.5030\nThe current learning rate: 0.00022\nEpoch 8500: train loss 3.4108, val loss 3.4127\nThe current learning rate: 0.00024\nEpoch 9000: train loss 3.3313, val loss 3.3313\nThe current learning rate: 0.00025\nEpoch 9500: train loss 3.2589, val loss 3.2571\nThe current learning rate: 0.00027\nEpoch 10000: train loss 3.1879, val loss 3.1895\nThe current learning rate: 0.00028\nEpoch 10500: train loss 3.1176, val loss 3.1207\nThe current learning rate: 0.00030\nEpoch 11000: train loss 3.0534, val loss 3.0601\nThe current learning rate: 0.00032\nEpoch 11500: train loss 2.9862, val loss 2.9892\nThe current learning rate: 0.00033\nEpoch 12000: train loss 2.9345, val loss 2.9452\nThe current learning rate: 0.00035\nEpoch 12500: train loss 2.8993, val loss 2.8982\nThe current learning rate: 0.00036\nEpoch 13000: train loss 2.8561, val loss 2.8579\nThe current learning rate: 0.00038\nEpoch 13500: train loss 2.7959, val loss 2.8045\nThe current learning rate: 0.00040\nEpoch 14000: train loss 2.7631, val loss 2.7625\nThe current learning rate: 0.00041\nEpoch 14500: train loss 2.7093, val loss 2.7103\nThe current learning rate: 0.00042\nEpoch 15000: train loss 2.6855, val loss 2.6813\nThe current learning rate: 0.00044\nEpoch 15500: train loss 2.6367, val loss 2.6374\nThe current learning rate: 0.00045\nEpoch 16000: train loss 2.6024, val loss 2.5994\nThe current learning rate: 0.00046\nEpoch 16500: train loss 2.5626, val loss 2.5712\nThe current learning rate: 0.00047\nEpoch 17000: train loss 2.5347, val loss 2.5413\nThe current learning rate: 0.00048\nEpoch 17500: train loss 2.5031, val loss 2.5074\nThe current learning rate: 0.00048\nEpoch 18000: train loss 2.4679, val loss 2.4748\nThe current learning rate: 0.00049\nEpoch 18500: train loss 2.4358, val loss 2.4441\nThe current learning rate: 0.00049\nEpoch 19000: train loss 2.4172, val loss 2.4261\nThe current learning rate: 0.00050\nEpoch 19500: train loss 2.3871, val loss 2.3938\nThe current learning rate: 0.00050\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ntrain_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\nvalidation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n\nplt.plot(train_loss_list_converted, 'g', label='train_loss')\nplt.plot(validation_loss_list_converted, 'r', label='validation_loss')\nplt.xlabel(\"Steps - Every 100 epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:21:32.568587Z","iopub.execute_input":"2025-06-24T18:21:32.568817Z","iopub.status.idle":"2025-06-24T18:21:32.801642Z","shell.execute_reply.started":"2025-06-24T18:21:32.568800Z","shell.execute_reply":"2025-06-24T18:21:32.800719Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"#Load the model\nmodel = GPT(config)  # re-create the model with same config\ndevice =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbest_model_params_path = \"best_model_params.pt\"\nmodel.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:21:32.802400Z","iopub.execute_input":"2025-06-24T18:21:32.802926Z","iopub.status.idle":"2025-06-24T18:21:32.963124Z","shell.execute_reply.started":"2025-06-24T18:21:32.802905Z","shell.execute_reply":"2025-06-24T18:21:32.962524Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"sentence = \"Once upon a time there was a pumpkin\"\ncontext = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\ny = model.generate(context, 200)\nprint(enc.decode(y.squeeze().tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:46:57.220550Z","iopub.execute_input":"2025-06-24T18:46:57.220853Z","iopub.status.idle":"2025-06-24T18:46:58.148914Z","shell.execute_reply.started":"2025-06-24T18:46:57.220829Z","shell.execute_reply":"2025-06-24T18:46:58.148280Z"}},"outputs":[{"name":"stdout","text":"Once upon a time there was a pumpkin. She was very careless and loved to play outside and explore. One day she confidently found some pretty flower and decided to go outside. She dug all it up high excited to the house and started to have a grape. The girl wanted to know where the muffin looked at it, so she reached out, and she quickly opened the hanger in birds.\n\nJust then, the pepperflower started sitting down, playing with her friends watching her. He didn't said she repaired it before pretending to be a hurt animal.\"\n\nLily drank her fur and smiled. She had never got your ice cream on mint, so she ran over to it with her mom. She said when she got to the beach, it was time for breakfast, and she watched different Nemo.\n\nThe ice- penguside and the girl were happy. They said thank you the jellyfish and covered vegetables together again. Fligy got some in their faces and became good friends discuss anything new together\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"sentence = \"A little girl went to the woods\"\ncontext = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\ny = model.generate(context, 200)\nprint(enc.decode(y.squeeze().tolist()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:21:34.081626Z","iopub.execute_input":"2025-06-24T18:21:34.081914Z","iopub.status.idle":"2025-06-24T18:21:35.051304Z","shell.execute_reply.started":"2025-06-24T18:21:34.081889Z","shell.execute_reply":"2025-06-24T18:21:35.050564Z"}},"outputs":[{"name":"stdout","text":"A little girl went to the woods, pretending to be a butterfly. She even thought that it was a special red flower! Everyone in the forest were going onA for she found it even more fancy bugs to pick up, but it was more fun.\n\nThe animals were so proud of them! They used it at Anne's heart. She ate the saus for the bee, who was even bigger. It made Kate laugh, even more sweoms away the rainbow!\n\nGeorge was so happy to escape of the wise puddle that bloom. He could soon speak to Kate instead and on the beautiful cactus and sang a the twigy danced. She even said \"Hello King!\".Alice and Betty had a brilliant idea.zie was out on the beach with her friends. They played slides and had so much fun. \n\nBut soon it's time to morning, every day they would go to the park and go back to their snow. The igloo Max was very sleepy. Soon, it was all\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# After model = GPT(config)\ndef count_model_parameters(model):\n    total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Model has {total:,} trainable parameters\")\n    return total\n\ntrainable_params = count_model_parameters(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:48:33.300309Z","iopub.execute_input":"2025-06-24T18:48:33.300581Z","iopub.status.idle":"2025-06-24T18:48:33.305927Z","shell.execute_reply.started":"2025-06-24T18:48:33.300561Z","shell.execute_reply":"2025-06-24T18:48:33.305293Z"}},"outputs":[{"name":"stdout","text":"Model has 29,995,392 trainable parameters\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# After training is complete\nprint(\"Training completed. Saving final model...\")\n\n# Save the complete model information\nfinal_model_data = {\n    'model_state_dict': model.state_dict(),\n    'config': config,\n    'best_val_loss': best_val_loss,\n    'training_config': {\n        'max_iters': max_iters,\n        'learning_rate': learning_rate,\n        'batch_size': batch_size,\n        'block_size': block_size\n    }\n}\n\n# Save complete model\ntorch.save(final_model_data, 'complete_trained_model.pt')\n\n# Also save just the state dict (lighter file)\ntorch.save(model.state_dict(), 'model_state_dict.pt')\n\n# Save the configuration separately as JSON for easy loading\nimport json\nconfig_dict = {\n    'vocab_size': config.vocab_size,\n    'block_size': config.block_size,\n    'n_layer': config.n_layer,\n    'n_head': config.n_head,\n    'n_embd': config.n_embd,\n    'dropout': config.dropout,\n    'bias': config.bias\n}\n\nwith open('model_config.json', 'w') as f:\n    json.dump(config_dict, f, indent=2)\n\nprint(\"Model saved successfully!\")\nprint(\"Files created:\")\nprint(\"- complete_trained_model.pt (full model + config)\")\nprint(\"- model_state_dict.pt (just weights)\")\nprint(\"- model_config.json (architecture config)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:49:14.922625Z","iopub.execute_input":"2025-06-24T18:49:14.922908Z","iopub.status.idle":"2025-06-24T18:49:15.374108Z","shell.execute_reply.started":"2025-06-24T18:49:14.922886Z","shell.execute_reply":"2025-06-24T18:49:15.373467Z"}},"outputs":[{"name":"stdout","text":"Training completed. Saving final model...\nModel saved successfully!\nFiles created:\n- complete_trained_model.pt (full model + config)\n- model_state_dict.pt (just weights)\n- model_config.json (architecture config)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Copy files to output directory for easy download\nimport shutil\nimport os\n\noutput_dir = '/kaggle/working'\nfiles_to_save = [\n    'complete_trained_model.pt',\n    'model_state_dict.pt', \n    'model_config.json',\n    'best_model_params.pt'\n]\n\nfor file in files_to_save:\n    if os.path.exists(file):\n        print(f\"File {file} size: {os.path.getsize(file) / (1024*1024):.2f} MB\")\n\nprint(\"\\nFiles are ready for download in the notebook output!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-24T18:49:19.148596Z","iopub.execute_input":"2025-06-24T18:49:19.149143Z","iopub.status.idle":"2025-06-24T18:49:19.154208Z","shell.execute_reply.started":"2025-06-24T18:49:19.149120Z","shell.execute_reply":"2025-06-24T18:49:19.153546Z"}},"outputs":[{"name":"stdout","text":"File complete_trained_model.pt size: 114.45 MB\nFile model_state_dict.pt size: 114.45 MB\nFile model_config.json size: 0.00 MB\nFile best_model_params.pt size: 114.45 MB\n\nFiles are ready for download in the notebook output!\n","output_type":"stream"}],"execution_count":26}]}